{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4ec7ce",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Fine-Tuning BLIP Using HuggingFace Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de5f4c",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293c4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Image as HFImage\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58911644",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset From CSV File\n",
    "dataset = load_dataset(\"csv\", data_files=r\"Dataset/metadata.csv\").cast_column(\"image\", HFImage())\n",
    "dataset  = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7bbac",
   "metadata": {},
   "source": [
    "### Dataset Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"text\"])\n",
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcceac",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90528579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b16ab",
   "metadata": {},
   "source": [
    "### Load Model And Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6428c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load From HuggingFace\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to('cuda')\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35bd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010cf5f",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "print(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314b913",
   "metadata": {},
   "source": [
    "### Saving The Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model-weights-base-finetuned.pth\")\n",
    "processor.save_pretrained(\"processor-config-base-finetuned.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18be718",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing test images for inference\n",
    "input_folder = r\"Dataset\\test\"\n",
    "\n",
    "\n",
    "# Load base model from Huggingface\n",
    "base_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to('cuda')\n",
    "\n",
    "\n",
    "# Load the fine-tuned model locally\n",
    "processor = BlipProcessor.from_pretrained(\"processor-config-base-finetuned.json\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"model-weights-base-finetuned.pth\").to('cuda')\n",
    "\n",
    "\n",
    "# Iterate through all files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "        # Open the image\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Perform inference on fine-tuned\n",
    "        inputs = processor(img, return_tensors=\"pt\").to('cuda')\n",
    "        out = model.generate(**inputs, max_new_tokens=50)\n",
    "        result = processor.decode(out[0], skip_special_tokens=True).replace(\"arafed\", \"\").strip()\n",
    "\n",
    "        # Perform inference on base\n",
    "        inputs = base_processor(img, return_tensors=\"pt\").to('cuda')\n",
    "        out = base_model.generate(**inputs, max_new_tokens=50)\n",
    "        result_base = base_processor.decode(out[0], skip_special_tokens=True).replace(\"arafed\", \"\").strip()\n",
    "\n",
    "        # Calculate dynamic text width based on image width\n",
    "        image_width, image_height = img.size\n",
    "        char_width = image_width // 60  # Adjust factor (60) for desired text size\n",
    "            \n",
    "        # Wrap the response text to ensure it fits\n",
    "        wrapped_result = \"\\n\".join(wrap(result, width=char_width))  # Adjust the width as necessary\n",
    "        wrapped_result_base = \"\\n\".join(wrap(result_base, width=char_width))  # Adjust the width as necessary\n",
    "\n",
    "        text = f\"Fine-Tuned Model Result:\\n{wrapped_result}\\n\\nBase Model Result:\\n{wrapped_result_base}\"\n",
    "\n",
    "        # Display the image and result\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\n",
    "            text,\n",
    "            fontsize=12,\n",
    "            fontweight='bold',\n",
    "            loc='center'\n",
    "            )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601c263",
   "metadata": {},
   "source": [
    "### Conditional Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"will start generating the caption from the text input. If no text input is provided, the decoder will start with the [BOS] token only.\"\"\"\n",
    "\n",
    "text = \"from\"\n",
    "inputs = processor(img, text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "out = model.generate(**inputs , max_new_tokens=50)\n",
    "print(processor.decode(out[0], skip_special_tokens=True).replace(\"arafed\", \"\").strip())\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
